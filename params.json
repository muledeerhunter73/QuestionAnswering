{"name":"Middle School Question Solver","tagline":"Can a Computer Be Smarter Than a Fifth Grader?","body":"\r\n### Introduction\r\nTo learn more about the AI field of natural language processing, I set out to create a program that uses NLP tools and techniques to process middle school exam history question and try to find the answer from Wikipedia.  Currently it returns the Wikipedia article with the answer, but I am working at having the system find the answer in the article.\r\n\r\n### So What is This?\r\nThis is a question answering system for middle school exam questions.  It specifically seeks to find answers to who, where and when questions. It is put together on an interactive website for users to access. The link to the website is included below. [muledeerhunter73.pythonanywhere.com](http://muledeerhunter73.pythonanywhere.com).\r\n\r\n### Why did I do this?\r\nNatural Language Processing fascinates me in that it allows computers in a sense to understand humans. I did this project to get my toes in the water of how natural language processing works.  Answers to middle school exam questions are abundant on the web, but they can be difficult to find in all the web content.  I decided to do this to try to get the computer to find the answer for me.\r\n\r\n### How to use the system?\r\nThe website is really simple to use.  Just go to the website above and type the question you have.  You do not need to worry about putting a question mark at the end of your question.  Also a helpful hint is to capitalize proper nouns, this helps the system in processing the question correctly. After you have entered your question you may need to wait a few seconds for the system to find the article.\r\n####Some Good Test Examples\r\n1. When were the moon landings?\r\n2. Who wrote the United States Declaration of Independence?\r\n3. Where is Mount Rushmore?\r\n\r\n### What is going on in the background?\r\nThe website is running a python script which is divided into three main sections.\r\n1 The first thing the system does is analyze your question into parts of speech.  It does this using a powerful toolkit called [nltk](http://nltk.org). Once it has tagged the question, it extracts the adjectives and nouns to use in the search query to Wikipedia.  \r\n2 In order to query the vast knowledge base of Wikipedia the system uses a [Wikipedia API](https://pypi.python.org/pypi/wikipedia/).  This works by using the API search functionality with the query to get a list of possible articles.  The system then goes through each of these articles and analyzes the title and first 20 sentences to try to determine the article that has the best chance of having the answer.  It does this by rating the article based on match words, phrases, and other similarities with the query. \r\n3 (This is still a work in progress) Once it has found the best article it will then analyze the article using the [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml) to find names, locations, and dates in the article depending on the type of question asked.  It will then analyze around these possible answers looking for words or patterns that match the query that this could be the answer. The system then returns the best answer it found. \r\n\r\n### Issues and Limitations\r\nThe system sometimes does not return the correct article. I have been having family and friends test the system so that I can try to find out why it returned the incorrect article and how I can improve it.\r\nI also have been trying to setup the system to find the answer.  There are great tools out there that can extract named entities such as names and locations.  The issue I have been having is getting these tools integrated into the site I am hosting. [Pythonanywhere](https://www.pythonanywhere.com/) is strictly python and many of these tools are written in other languages and include a python interface.\r\n\r\n### Authors and Contributors\r\nGitHub Profile Name: muledeerhunter73\r\nHadley Westover\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}